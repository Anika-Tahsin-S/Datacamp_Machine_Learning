{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arithmetical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 123)\n",
    "\n",
    "def get_kfold_rmse(train):\n",
    "    mse_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(train):\n",
    "        train = train.fillna(0)\n",
    "        feats = [x for x in train.columns if x not in ['Id', 'SalePrice', 'RoofStyle', 'CentralAir']]\n",
    "        \n",
    "        fold_train, fold_test = train.loc[train_index], train.loc[test_index]\n",
    "\n",
    "        # Fit the data and make predictions\n",
    "        # Create a Random Forest object\n",
    "        rf = RandomForestRegressor(n_estimators=10, min_samples_split=10, random_state=123)\n",
    "\n",
    "        # Train a model\n",
    "        rf.fit(X=fold_train[feats], y=fold_train['SalePrice'])\n",
    "\n",
    "        # Get predictions for the test set\n",
    "        pred = rf.predict(fold_test[feats])\n",
    "    \n",
    "        fold_score = mean_squared_error(fold_test['SalePrice'], pred)\n",
    "        mse_scores.append(np.sqrt(fold_score))\n",
    "        \n",
    "    return round(np.mean(mse_scores) + np.std(mse_scores), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('house_prices_train.csv')\n",
    "test = pd.read_csv('house_prices_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE before feature engineering: 34413.55\n",
      "RMSE with total area: 34413.55\n",
      "RMSE with garden area: 34413.55\n",
      "RMSE with number of bathrooms: 34506.78\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "# Look at the initial RMSE\n",
    "print('RMSE before feature engineering:', get_kfold_rmse(train))\n",
    "\n",
    "# Find the total area of the house\n",
    "train['TotalArea'] = train['TotalBsmtSF'] + train['FirstFlrSF'] + train['SecondFlrSF']\n",
    "\n",
    "# Look at the updated RMSE\n",
    "print('RMSE with total area:', get_kfold_rmse(train))\n",
    "\n",
    "# Part 2\n",
    "# Find the area of the garden\n",
    "train['GardenArea'] = train['LotArea'] - train['FirstFlrSF']\n",
    "print('RMSE with garden area:', get_kfold_rmse(train))\n",
    "\n",
    "\n",
    "# Part 3\n",
    "# Find total number of bathrooms\n",
    "train['TotalBath'] = train['FullBath'] + train['HalfBath']\n",
    "print('RMSE with number of bathrooms:', get_kfold_rmse(train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('taxi_train_chapter_4.csv')\n",
    "test = pd.read_csv('taxi_test_chapter_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train and test together\n",
    "taxi = pd.concat([train, test])\n",
    "\n",
    "# Convert pickup date to datetime object\n",
    "taxi['pickup_datetime'] = pd.to_datetime(taxi['pickup_datetime'])\n",
    "\n",
    "# Create a day of week feature\n",
    "taxi['dayofweek'] = taxi['pickup_datetime'].dt.dayofweek\n",
    "\n",
    "# Create an hour feature\n",
    "taxi['hour'] = taxi['pickup_datetime'].dt.hour\n",
    "\n",
    "# Split back into train and test\n",
    "new_train = taxi[taxi['id'].isin(train['id'])]\n",
    "new_test = taxi[taxi['id'].isin(test['id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('house_prices_train.csv')\n",
    "test = pd.read_csv('house_prices_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  RoofStyle  RoofStyle_enc CentralAir  CentralAir_enc\n",
      "0     Gable              1          Y               1\n",
      "1     Gable              1          Y               1\n",
      "2     Gable              1          Y               1\n",
      "3     Gable              1          Y               1\n",
      "4     Gable              1          Y               1\n"
     ]
    }
   ],
   "source": [
    "# Concatenate train and test together\n",
    "houses = pd.concat([train, test])\n",
    "\n",
    "# Label encoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Create new features\n",
    "houses['RoofStyle_enc'] = le.fit_transform(houses[\"RoofStyle\"])\n",
    "houses['CentralAir_enc'] = le.fit_transform(houses[\"CentralAir\"])\n",
    "\n",
    "# Look at new features\n",
    "print(houses[['RoofStyle', 'RoofStyle_enc', 'CentralAir', 'CentralAir_enc']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gable      2310\n",
      "Hip         551\n",
      "Gambrel      22\n",
      "Flat         20\n",
      "Mansard      11\n",
      "Shed          5\n",
      "Name: RoofStyle, dtype: int64 \n",
      "\n",
      "Y    2723\n",
      "N     196\n",
      "Name: CentralAir, dtype: int64\n",
      "  RoofStyle  RoofStyle_Flat  RoofStyle_Gable  RoofStyle_Gambrel  \\\n",
      "0     Gable               0                1                  0   \n",
      "1     Gable               0                1                  0   \n",
      "2     Gable               0                1                  0   \n",
      "\n",
      "   RoofStyle_Hip  RoofStyle_Mansard  RoofStyle_Shed  \n",
      "0              0                  0               0  \n",
      "1              0                  0               0  \n",
      "2              0                  0               0  \n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "# Concatenate train and test together\n",
    "houses = pd.concat([train, test])\n",
    "\n",
    "# Look at feature distributions\n",
    "print(houses['RoofStyle'].value_counts(), '\\n')\n",
    "print(houses['CentralAir'].value_counts())\n",
    "\n",
    "# Part 2\n",
    "# Which of the features is binary?\n",
    "# Answer: \"CentralAir\".\n",
    "\n",
    "# Part 3\n",
    "le = LabelEncoder()\n",
    "houses['CentralAir_enc'] = le.fit_transform(houses['CentralAir'])\n",
    "\n",
    "# Part 4\n",
    "# Create One-Hot encoded features\n",
    "ohe = pd.get_dummies(houses['RoofStyle'], prefix = 'RoofStyle')\n",
    "\n",
    "# Concatenate OHE features to houses\n",
    "houses = pd.concat([houses, ohe], axis=1)\n",
    "\n",
    "# Look at OHE features\n",
    "print(houses[[col for col in houses.columns if 'RoofStyle' in col]].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1\n",
    "def test_mean_target_encoding(train, test, target, categorical, alpha = 5):\n",
    "    # Calculate global mean on the train data\n",
    "    global_mean = train[target].mean()\n",
    "    \n",
    "    # Group by the categorical feature and calculate its properties\n",
    "    train_groups = train.groupby(categorical)\n",
    "    category_sum = train_groups[target].sum()\n",
    "    category_size = train_groups.size()\n",
    "    \n",
    "    # Calculate smoothed mean target statistics\n",
    "    train_statistics = (category_sum + global_mean * alpha) / (category_size + alpha)\n",
    "    \n",
    "    # Apply statistics to the test data and fill new categories\n",
    "    test_feature = test[categorical].map(train_statistics).fillna(global_mean)\n",
    "    return test_feature.values\n",
    "\n",
    "\n",
    "# Part 2\n",
    "def train_mean_target_encoding(train, target, categorical, alpha = 5):\n",
    "    # Create 5-fold cross-validation\n",
    "    kf = KFold(n_splits = 5, random_state = 123, shuffle = True)\n",
    "    train_feature = pd.Series(index=train.index)\n",
    "    \n",
    "    # For each folds split\n",
    "    for train_index, test_index in kf.split(train):\n",
    "        cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
    "      \n",
    "        # Calculate out-of-fold statistics and apply to cv_test\n",
    "        cv_test_feature = test_mean_target_encoding(cv_train, cv_test, target, categorical, alpha)\n",
    "        \n",
    "        # Save new feature for this particular fold\n",
    "        train_feature.iloc[test_index] = cv_test_feature       \n",
    "    return train_feature.values\n",
    "\n",
    "\n",
    "# Part 3\n",
    "def mean_target_encoding(train, test, target, categorical, alpha = 5):\n",
    "  \n",
    "    # Get the train feature\n",
    "    train_feature = train_mean_target_encoding(train, target, categorical, alpha)\n",
    "  \n",
    "    # Get the test feature\n",
    "    test_feature = test_mean_target_encoding(train, test, target, categorical, alpha)\n",
    "    \n",
    "    # Return new features to add to the model\n",
    "    return train_feature, test_feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bryant_shots = pd.read_csv('bryant_shots.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       game_id  shot_made_flag  game_id_enc\n",
      "9707  20700101             0.0     0.461172\n",
      "       game_id  shot_made_flag  game_id_enc\n",
      "3703  20200806             0.0     0.370094\n",
      "      game_id  shot_made_flag  game_id_enc\n",
      "829  20000651             1.0     0.281348\n",
      "       game_id  shot_made_flag  game_id_enc\n",
      "5097  20301110             0.0      0.36457\n",
      "       game_id  shot_made_flag  game_id_enc\n",
      "5595  20400328             1.0     0.590904\n"
     ]
    }
   ],
   "source": [
    "# Create 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, random_state=123, shuffle=True)\n",
    "\n",
    "# For each folds split\n",
    "for train_index, test_index in kf.split(bryant_shots):\n",
    "    cv_train, cv_test = bryant_shots.iloc[train_index].copy(), bryant_shots.iloc[test_index].copy()\n",
    "    \n",
    "    # Create mean target encoded feature\n",
    "    cv_train['game_id_enc'], cv_test['game_id_enc'] = mean_target_encoding(train=cv_train,\n",
    "                                                                           test=cv_test,\n",
    "                                                                           target='shot_made_flag',\n",
    "                                                                           categorical='game_id',\n",
    "                                                                           alpha=5)\n",
    "    \n",
    "    # Look at the encoding\n",
    "    print(cv_train[['game_id', 'shot_made_flag', 'game_id_enc']].sample(n = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('house_prices_train.csv')\n",
    "test = pd.read_csv('house_prices_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     RoofStyle  RoofStyle_enc\n",
      "0        Gable  171565.947836\n",
      "1          Hip  217594.645131\n",
      "98     Gambrel  164152.950424\n",
      "133       Flat  188703.563431\n",
      "362    Mansard  180775.938759\n",
      "1053      Shed  188267.663242\n"
     ]
    }
   ],
   "source": [
    "# Create mean target encoded feature\n",
    "train['RoofStyle_enc'], test['RoofStyle_enc'] = mean_target_encoding(train = train,\n",
    "                                                                     test = test,\n",
    "                                                                     target = 'SalePrice',\n",
    "                                                                     categorical = 'RoofStyle',\n",
    "                                                                     alpha = 10)\n",
    "\n",
    "# Look at the encoding\n",
    "print(test[['RoofStyle', 'RoofStyle_enc']].drop_duplicates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                 0\n",
      "bathrooms          0\n",
      "bedrooms           0\n",
      "building_id       13\n",
      "latitude           0\n",
      "longitude          0\n",
      "manager_id         0\n",
      "price             32\n",
      "interest_level     0\n",
      "dtype: int64\n",
      "                        building_id   price\n",
      "0  53a5b119ba8f7b61d4e010512e0dfc85  3000.0\n",
      "1  c5c8a357cba207596b04d1afd1e4f130  5465.0\n",
      "2  c3ba40552e2120b0acfc3cb5730bb2aa  2850.0\n",
      "3  28d9ad350afeaab8027513a3e52ac8d5  3275.0\n",
      "4                               NaN  3350.0\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "# Read dataframe\n",
    "rental_listings = pd.read_csv('twosigma_train.csv')\n",
    "\n",
    "# Find the number of missing values in each column\n",
    "print(rental_listings.isnull().sum())\n",
    "\n",
    "# Part 2\n",
    "# Look at the columns with the missing values\n",
    "print(rental_listings[['building_id', 'price']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1\n",
    "# Import SimpleImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create mean imputer\n",
    "mean_imputer = SimpleImputer(strategy = 'mean')\n",
    "\n",
    "# Price imputation\n",
    "rental_listings[['price']] = mean_imputer.fit_transform(rental_listings[['price']])\n",
    "\n",
    "\n",
    "# Part 2\n",
    "# Import SimpleImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create constant imputer\n",
    "constant_imputer = SimpleImputer(strategy = 'constant', fill_value = 'MISSING')\n",
    "\n",
    "# building_id imputation\n",
    "rental_listings[['building_id']] = constant_imputer.fit_transform(rental_listings[['building_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                0\n",
       "bathrooms         0\n",
       "bedrooms          0\n",
       "building_id       0\n",
       "latitude          0\n",
       "longitude         0\n",
       "manager_id        0\n",
       "price             0\n",
       "interest_level    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rental_listings.isnull().sum()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b89b5cfaba6639976dc87ff2fec6d58faec662063367e2c229c520fe71072417"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
